---
layout: post
title: "ü§ñ Working with LLMs Part 5 - Build an AI Assistant Using Tool Calling üöÄ"
date: 2025-10-17 10:00:00 +0000
categories: [LLM, Gen AI, Ollama, Local AI Models]
tags: [LLM, Gen AI, Ollama, Local AI Models]
author_profile: true
author: "nagul_meera"
reading_time: 5
excerpt: "Imagine asking an AI: What's the weather in Tokyo? The AI doesn't actually know the current weather‚Äîit needs to call an external function to fetch real-time data. This is where tool calling (also known as function calling) becomes powerful."
---
## üì∫ Tutorial Video

- [**ü§ñ Working with LLMs Part 5: Build an AI Assistant Using Tool Calling üöÄ**](https://youtu.be/jCMPxct7ARc){:target="_blank"}
  [![ü§ñ Working with LLMs Part 5: Build an AI Assistant Using Tool Calling üöÄ](/docs/assets/images/2025/oct/llms/5.png)](https://youtu.be/jCMPxct7ARc){:target="_blank"}
---
## üéØ Introduction to Tool Calling

Imagine asking an AI: *"What's the weather in Tokyo?"* The AI doesn't actually know the current weather‚Äîit needs to call an external function to fetch real-time data. This is where **tool calling** (also known as function calling) becomes powerful.

**Tool calling** enables Large Language Models (LLMs) to:
- üîß Execute external functions
- üìä Access real-time data
- üóÑÔ∏è Query databases
- üìÅ Read files from your system
- üåê Make API calls

Instead of just generating text, LLMs can now **take actions** and **retrieve information** dynamically, making them truly useful assistants.

---

## üí° Use Case: Building an AI Documentation Assistant

### The Problem
You have dozens of markdown documentation files scattered across directories. Finding specific information requires:
- üîç Manually searching through files
- üìÇ Remembering which file contains what
- ‚è∞ Wasting time on repetitive lookups

### The Solution
Build an AI assistant that:
- ‚úÖ Understands your questions in natural language
- ‚úÖ Automatically finds and reads relevant documentation
- ‚úÖ Provides accurate answers from your local files
- ‚úÖ Saves time and improves productivity

### Real-World Scenario

**Imagine you're a developer working on a project with extensive documentation:**

üìö **Your Documentation:**
```
docs/
‚îú‚îÄ‚îÄ installation-guide.md
‚îú‚îÄ‚îÄ api-reference.md
‚îú‚îÄ‚îÄ authentication.md
‚îú‚îÄ‚îÄ deployment-guide.md
‚îú‚îÄ‚îÄ troubleshooting.md
‚îî‚îÄ‚îÄ best-practices.md
```

**Traditional Approach (Without AI Assistant):**
1. üòì Think "Where did I document the authentication process?"
2. üîç Open file explorer, browse through files
3. üìñ Open authentication.md, scan through content
4. ‚è∞ Spend 5-10 minutes finding the specific section
5. üîÑ Repeat for every question

**With AI Documentation Assistant:**
1. üí¨ Ask: "How do I authenticate users in the API?"
2. ‚ö° AI instantly reads authentication.md
3. üéØ Gets precise answer in seconds
4. ‚úÖ Continue coding without context switching

### Use Case Benefits

| Without AI Assistant | With AI Assistant |
|---------------------|-------------------|
| üò´ Manual file searching | ü§ñ Automatic file detection |
| ‚è∞ 5-10 minutes per lookup | ‚ö° Instant answers |
| üß† Need to remember file structure | üí¨ Just ask in plain English |
| üìñ Read entire documents | üéØ Get specific information |
| üîÑ Context switching disrupts flow | üöÄ Stay in your workflow |

### Who Benefits?

- üë®‚Äçüíª **Developers** - Quick access to API docs and guides
- üìù **Technical Writers** - Verify documentation content
- üéì **New Team Members** - Learn codebase faster
- üîß **DevOps Engineers** - Find deployment procedures
- üë• **Support Teams** - Answer customer questions accurately

---

## üîÑ How It Works: The Flow

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ    User     ‚îÇ "What's in the API guide?"
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Streamlit UI      ‚îÇ Captures user question
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Ollama LLM        ‚îÇ Analyzes question + available tools
‚îÇ   + Tool Defs       ‚îÇ Decides: "I need to read api-guide.md"
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Tool Function     ‚îÇ Executes: read_file("api-guide.md")
‚îÇ   (Python)          ‚îÇ Returns: File content
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Ollama LLM        ‚îÇ Processes file content
‚îÇ   (with context)    ‚îÇ Generates natural answer
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   User Gets Answer  ‚îÇ "The API guide covers authentication,
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  endpoints, and rate limits..."
```

---

## üõ†Ô∏è Implementation: Key Components

### 1Ô∏è‚É£ Define Tools (JSON Schema)

First, define what functions your LLM can call:

```python
tools = [
    {
        "type": "function",
        "function": {
            "name": "read_file",
            "description": "Read the complete content of a markdown file",
            "parameters": {
                "type": "object",
                "properties": {
                    "file_name": {
                        "type": "string",
                        "description": "Name of the markdown file (e.g., 'api-guide.md')"
                    }
                },
                "required": ["file_name"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "list_all_files",
            "description": "List all available markdown files",
            "parameters": {
                "type": "object",
                "properties": {},
                "required": []
            }
        }
    }
]
```

**Key Points:**
- üìù Each tool has a clear `name` and `description`
- üìã Parameters are defined using JSON Schema
- üéØ Descriptions help the LLM decide when to use each tool

---

### 2Ô∏è‚É£ Implement Tool Functions

Create the actual Python functions that do the work:

```python
from pathlib import Path

def read_file(file_name: str) -> str:
    """Read and return markdown file content"""
    docs_dir = Path("./docs")
    file_path = docs_dir / file_name
    
    if not file_path.exists():
        return f"‚ùå File not found: {file_name}"
    
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    return f"üìÑ Content of {file_name}:\n\n{content}"

def list_all_files() -> str:
    """List all markdown files in directory"""
    docs_dir = Path("./docs")
    md_files = list(docs_dir.glob('**/*.md'))
    
    result = f"üìö Found {len(md_files)} file(s):\n\n"
    for file in md_files:
        result += f"  ‚Ä¢ {file.name}\n"
    
    return result

# Map function names to implementations
available_functions = {
    'read_file': read_file,
    'list_all_files': list_all_files
}
```

---

### 3Ô∏è‚É£ Call LLM with Tools

Send user message and tool definitions to the LLM:

```python
import ollama

# User asks a question
user_message = "What's in the installation guide?"

# Call LLM with tools available
response = ollama.chat(
    model='llama3.2',
    messages=[{'role': 'user', 'content': user_message}],
    tools=tools
)

print(response['message'])
```

**What happens:**
- üß† LLM analyzes the question
- üéØ Decides it needs to call `read_file("installation.md")`
- üì§ Returns a tool call request

---

### 4Ô∏è‚É£ Execute Tool Calls

When the LLM requests a tool, execute it:

```python
# Check if LLM wants to call a tool
if response['message'].get('tool_calls'):
    # Prepare messages list
    messages = [{'role': 'user', 'content': user_message}]
    messages.append(response['message'])
    
    # Execute each tool call
    for tool in response['message']['tool_calls']:
        function_name = tool['function']['name']
        arguments = tool['function']['arguments']
        
        # Call the actual function
        function_to_call = available_functions[function_name]
        function_result = function_to_call(**arguments)
        
        # Add result to conversation
        messages.append({
            'role': 'tool',
            'content': function_result
        })
    
    # Get final response from LLM with tool results
    final_response = ollama.chat(
        model='llama3.2',
        messages=messages
    )
    
    print(final_response['message']['content'])
```

---

### 5Ô∏è‚É£ Build the UI with Streamlit

Create an interactive chat interface:

```python
import streamlit as st

st.title("üí¨ Documentation Assistant")

# Chat input
if prompt := st.chat_input("Ask about your documentation..."):
    # Add user message
    st.session_state.messages.append({"role": "user", "content": prompt})
    
    # Get response with tool calling logic
    response = get_llm_response(prompt)
    
    # Display response
    st.session_state.messages.append({"role": "assistant", "content": response})
```

---

## üé® Real-World Example

**User Input:**
```
"How do I authenticate with the API?"
```

**Behind the Scenes:**

1. **LLM Decision:**
   ```json
   {
     "tool_call": "read_file",
     "arguments": {"file_name": "api-guide.md"}
   }
   ```

2. **Tool Execution:**
   ```python
   # Reads api-guide.md from disk
   content = """
   # API Authentication
   
   Use Bearer token authentication:
   
   Headers:
     Authorization: Bearer YOUR_API_KEY
   """
   ```

3. **LLM Response:**
   ```
   To authenticate with the API, you need to use Bearer token 
   authentication. Include your API key in the Authorization 
   header as follows:
   
   Authorization: Bearer YOUR_API_KEY
   
   This is documented in the API guide's authentication section.
   ```

---

## ‚ú® Key Benefits

| Feature | Benefit |
|---------|---------|
| üéØ **Natural Language** | Ask questions in plain English |
| üöÄ **Automated Search** | No manual file hunting |
| üìö **Context-Aware** | Understands entire documentation |
| üîÑ **Real-Time** | Always reads latest file content |
| üé® **User-Friendly** | Chat interface anyone can use |

---

## üö¶ When to Use Tool Calling

‚úÖ **Good Use Cases:**
- üìÅ Reading files/documents
- üåê Fetching real-time data (weather, stocks, news)
- üóÑÔ∏è Querying databases
- üìä Running calculations
- üîç Searching external APIs

‚ùå **Not Needed For:**
- üí≠ General knowledge questions
- ü§î Creative writing
- üìñ Explaining concepts
- üí° Brainstorming ideas

---

## üéØ Quick Start Guide

```bash
# 1. Install dependencies
pip install streamlit ollama

# 2. Pull an Ollama model
ollama pull llama3.2

# 3. Create your tool definitions and functions

# 4. Run your app
streamlit run app.py
```

---

## üîë Key Takeaways

1. **Tool calling bridges the gap** between LLM intelligence and real-world data
2. **Define clear tool schemas** so the LLM knows when and how to use them
3. **Implement reliable functions** that return structured, useful data
4. **Combine multiple tools** to create powerful AI assistants
5. **The LLM decides** which tools to use based on user questions

---

## üöÄ What's Next?

Now that you understand tool calling, you can:
- üåê Add API integrations (weather, news, databases)
- üìä Create data analysis assistants
- ü§ñ Build autonomous agents
- üîó Chain multiple tool calls together
- üìà Scale to production applications

**Tool calling transforms LLMs from text generators into intelligent agents that can take action!**

---

## üìö Resources
- [GitHub Repository - AI Documentation Assistant](https://github.com/AI-Code-Geek/llms-tool-calling-ai-assistant)
- [Ollama Documentation](https://ollama.ai/docs)
- [Anthropic Tool Use Guide](https://docs.anthropic.com/claude/docs/tool-use)
- [OpenAI Function Calling](https://platform.openai.com/docs/guides/function-calling)
- [Streamlit Documentation](https://docs.streamlit.io)

---

*Ready to build your own AI assistant? Start with a simple use case and expand from there. The possibilities are endless! üöÄ*